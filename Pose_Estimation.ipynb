{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtqjqTe-Xgx7"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5_WG-8sXkI9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILqUYFqGXxXI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import time\n",
        "from scipy import spatial\n",
        "import math\n",
        "from math import hypot\n",
        "from google.colab.patches import cv2_imshow # if using colab\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhL3S8Y0XzBJ"
      },
      "outputs": [],
      "source": [
        "# initialize mediapipe requirements\n",
        "mpPose = mp.solutions.pose\n",
        "pose = mpPose.Pose()\n",
        "mpDraw = mp.solutions.drawing_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QESnJ_ItZ27I"
      },
      "outputs": [],
      "source": [
        "# input video path\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/CSE_237D/rock_dataset_0/clip3/climb.mp4')\n",
        "\n",
        "# input ground truth excel file name\n",
        "excel_path = 'Climb Seconds.xlsx'\n",
        "\n",
        "# input clip name, format: Clip 2 -> the format has to correspond with the sheet name in excel file\n",
        "clip_name = 'Clip 3'\n",
        "\n",
        "# define the cosine similarity threshold \n",
        "threshold = 0.9999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSG0Pn0YnVoE"
      },
      "outputs": [],
      "source": [
        "# global dict to store the coordiantes (required towards MVP)\n",
        "dict_coordinates = {'left_hand': [], 'right_hand': [], 'left_leg': [], 'right_leg': [], 'left_hip': [], 'right_hip': []}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute landmarks for a frame\n",
        "def find_pose(img):\n",
        "  break_signal = False\n",
        "  results = []\n",
        "  try:\n",
        "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(imgRGB)\n",
        "    print('Landmarks:', results.pose_landmarks)\n",
        "  except:\n",
        "    break_signal = True\n",
        "\n",
        "  return img, results, break_signal"
      ],
      "metadata": {
        "id": "9pKt-bIyYjsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL5P_5K-nYOK"
      },
      "outputs": [],
      "source": [
        "# retrieve coordinates from lm_list and store the coordinates in the global dict\n",
        "def store_coordinates(lm_list):\n",
        "  global dict_coordinates\n",
        "  dict_coordinates['left_hand'].append((lm_list[38], lm_list[39])) #left_index - x, y \n",
        "  dict_coordinates['right_hand'].append((lm_list[40], lm_list[41])) #right_index - x, y\n",
        "  dict_coordinates['left_hip'].append((lm_list[46], lm_list[47])) #left_hip - x, y\n",
        "  dict_coordinates['right_hip'].append((lm_list[48], lm_list[49])) #right_hip - x, y\n",
        "  dict_coordinates['left_leg'].append((lm_list[62], lm_list[63])) #left_foot - x, y\n",
        "  dict_coordinates['right_leg'].append((lm_list[64], lm_list[65])) #right_foot - x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf8j-SI7nW4u"
      },
      "outputs": [],
      "source": [
        "# compute cosine smilarity between two lm lists \n",
        "def check_similarity(list1, list2):\n",
        "  result = 1 - spatial.distance.cosine(list1, list2)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the image only when the frames are dissimilar\n",
        "def plot_image(img, results, cx, cy, elapsed_time):\n",
        "  mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n",
        "  cv2.circle(img, (cx, cy), 5, (255,0, 150), cv2.FILLED)\n",
        "  cv2.putText(img, str(int(elapsed_time)), (50, 50), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 3)\n",
        "  cv2_imshow(img) # if using colab\n",
        "  # cv2.imshow('image', img)\n",
        "  cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "UvZveavrx7Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_distance(list1, list2):\n",
        "  list_1_left_x = list1[46]\n",
        "  list_1_left_y = list1[47]\n",
        "  list_2_left_x = list2[46]\n",
        "  list_2_left_y = list2[46]\n",
        "  left_dist = math.hypot(list_1_left_x-list_2_left_x, list_1_left_y-list_2_left_y)\n",
        "  list_1_right_x = list1[48]\n",
        "  list_1_right_y = list1[49]\n",
        "  list_2_right_x = list2[48]\n",
        "  list_2_right_y = list2[49]\n",
        "  right_dist = math.hypot(list_1_right_x-list_2_right_x, list_1_right_y-list_2_right_y)\n",
        "  return (left_dist + right_dist)/2"
      ],
      "metadata": {
        "id": "VRvP-1ZB9XWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_coord_bounding(lm_list, holds_bounding_box):\n",
        "  if check_point_in_box(lm_list[38], lm_list[39], holds_bounding_box) and check_point_in_box(lm_list[40], lm_list[41], holds_bounding_box) and check_point_in_box(lm_list[62], lm_list[63], holds_bounding_box) and check_point_in_box(lm_list[64], lm_list[65], holds_bounding_box):\n",
        "      return True\n"
      ],
      "metadata": {
        "id": "-1PzY08EC3wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_point_in_box(x,y, holds_bounding_box):\n",
        "  return True"
      ],
      "metadata": {
        "id": "bOMJaEzgV0-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load ground truth labels from excel \n",
        "def load_from_excel(excel_path):\n",
        "  excel_pd = pd.ExcelFile(excel_path)\n",
        "  raw_ground_truth_dict = {sheet : excel_pd.parse(sheet)['Timings'].tolist() for sheet in excel_pd.sheet_names}\n",
        "  # print(raw_ground_truth_dict['Clip 2'])\n",
        "  return raw_ground_truth_dict"
      ],
      "metadata": {
        "id": "j6Ct9z07Zr17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute accuracy of predictions\n",
        "def compute_scores(raw_predictions, raw_ground_truth_labels, total_elapsed_time):\n",
        "\n",
        "  # masked attributes are those which can be directly used with sklearn metric functions to compute the scores\n",
        "  masked_predictions = [0 for time in range(total_elapsed_time + 1)]\n",
        "  masked_ground_truth_labels = [0 for time in range(total_elapsed_time + 1)]\n",
        "\n",
        "  # perform masking; don't forget to add 1 to total_elapsed_time when using range()\n",
        "  for time in range(total_elapsed_time + 1):\n",
        "    if time in raw_predictions:\n",
        "      masked_predictions[time] = 1\n",
        "    \n",
        "    if time in raw_ground_truth_labels:\n",
        "      masked_ground_truth_labels[time] = 1\n",
        "\n",
        "  print('masked_predictions: ', masked_predictions)\n",
        "  print('masked_ground_truth_labels: ', masked_ground_truth_labels)\n",
        "\n",
        "  # the format for using sklearn scores is: score(y_true, y_pred)\n",
        "  accuracy = accuracy_score(masked_ground_truth_labels, masked_predictions)\n",
        "  precision = precision_score(masked_ground_truth_labels, masked_predictions)\n",
        "  recall = recall_score(masked_ground_truth_labels, masked_predictions)\n",
        "  f1score = f1_score(masked_ground_truth_labels, masked_predictions)\n",
        "\n",
        "  return accuracy, precision, recall, f1score\n"
      ],
      "metadata": {
        "id": "vHf6JUizXEke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UeGf37EaK2g"
      },
      "outputs": [],
      "source": [
        "def main(cap):\n",
        "  prev = []\n",
        "  output_img_list = []\n",
        "  first_frame_flag = True\n",
        "  stored_frames_count = 0\n",
        "  pTime = 0\n",
        "  total_distance = 0\n",
        "  num_moves = 0\n",
        "  holds_bounding_box = []\n",
        "  # required for computing accuracy\n",
        "  raw_predictions = []\n",
        "\n",
        "  # required for time elapased and accuracy computation\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS) # note that this fps is constant (we shot the videos at 30 fps)\n",
        "\n",
        "  while True:\n",
        "    print('----------------------')\n",
        "    print('Processing a new frame')\n",
        "    success, img = cap.read()\n",
        "    img, results, main_break_signal = find_pose(img)\n",
        "    \n",
        "    # the signal means that there are no more input frames in the video, and thus the code must terminate\n",
        "    if (main_break_signal == True):\n",
        "      break\n",
        "    \n",
        "    frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "    elapsed_time = frame_number / fps\n",
        "    # cTime = time.time()\n",
        "    # fps = 1 / (cTime - pTime)\n",
        "    # pTime = cTime\n",
        "    print('Elapsed time: ', elapsed_time)\n",
        "    lm_list = []\n",
        "\n",
        "    if results.pose_landmarks:     \n",
        "      # add all 66 cordinates to lm_list\n",
        "      for id, lm in enumerate(results.pose_landmarks.landmark):  \n",
        "        h, w, c = img.shape\n",
        "        cx, cy = int(lm.x*w), int(lm.y*h)\n",
        "        lm_list.append(cx)\n",
        "        lm_list.append(cy)\n",
        "\n",
        "      # for the first frame, compute and store the coordinates\n",
        "      if(first_frame_flag == True):\n",
        "        store_coordinates(lm_list)\n",
        "        print(\"Similarity found for the first frame and coordinates stored\")\n",
        "        prev = lm_list\n",
        "        first_frame_flag = False\n",
        "        stored_frames_count += 1\n",
        "        output_img_list.append(img)\n",
        "        plot_image(img, results, cx, cy, elapsed_time)\n",
        "        raw_predictions.append(int(elapsed_time)) # conversion to int just cuts the decimal part, which is actually the intended behavior followed by ground truth videos\n",
        "\n",
        "\n",
        "      # from next frame onwards, first check similarity and then store the coordinates\n",
        "      else:\n",
        "        result = check_similarity(prev, lm_list) #prev = 66 cordinates, lm_list = 66 cordinates\n",
        "        print('Similarity Value:', result)\n",
        "        if(result < threshold):\n",
        "          store_coordinates(lm_list)\n",
        "          if check_coord_bounding(lm_list, holds_bounding_box):\n",
        "            total_distance += compute_distance(prev, lm_list)\n",
        "            num_moves = num_moves + 1\n",
        "          print(\"Similarity found and coordinates stored\")\n",
        "          stored_frames_count += 1\n",
        "          output_img_list.append(img)\n",
        "          plot_image(img, results, cx, cy, elapsed_time)\n",
        "          raw_predictions.append(int(elapsed_time)) # conversion to int just cuts the decimal part, which is actually the intended behavior followed by ground truth videos\n",
        "        \n",
        "        prev = lm_list\n",
        "          \n",
        "      print('Prev list: ', prev)\n",
        "      print('Length of prev list: ', len(prev))\n",
        "      print('LM list: ', lm_list)\n",
        "      print('Length of lm_list: ', len(lm_list))\n",
        "\n",
        "  \n",
        "  # compute the total time elapsed -> required for masking the raw predictions and raw ground truth labels\n",
        "  total_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  total_elapsed_time = total_frame_count/fps\n",
        "  \n",
        "  print('---------- Processsing Completed ----------')\n",
        "  print('Total elapsed time: ', total_elapsed_time)\n",
        "  print('Total frames processed: ', total_frame_count)\n",
        "  print('Total frames stored: ', stored_frames_count)\n",
        "  print('Total distance covered (in pixels): ', total_distance)\n",
        "  print('Number of moves: ', num_moves)\n",
        "  \n",
        "  # output a video consisting of just the processed frames\n",
        "  width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float 'width'\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float 'height'\n",
        "  img_size = (width, height)\n",
        "  fps_output = 5\n",
        "  output_video = cv2.VideoWriter('pose_estimation.mp4',cv2.VideoWriter_fourcc(*'mp4v'), fps_output, img_size)\n",
        " \n",
        "  for i in range(len(output_img_list)):\n",
        "    output_video.write(output_img_list[i])\n",
        "  \n",
        "  output_video.release()\n",
        "  print('Output Video Released: ', fps_output, 'fps')\n",
        "\n",
        "  # computing accuracy\n",
        "  # load the ground truth from excel\n",
        "  raw_ground_truth_dict = load_from_excel(excel_path) # each key of the raw_ground_truth_dict represents a clip and the corresponding value is the list of timesteps\n",
        "  \n",
        "  # retieve ground truth labels (from raw_ground_truth_dict) only for the current clip \n",
        "  raw_ground_truth_labels = raw_ground_truth_dict[clip_name]\n",
        "\n",
        "  # retrieve the evaluation metric scores\n",
        "  accuracy, precision, recall, f1_score = compute_scores(raw_predictions, raw_ground_truth_labels, int(total_elapsed_time))\n",
        "  print('Cosine similarity threshold used: ', threshold)\n",
        "  print('Accuracy: ', accuracy)\n",
        "  print('Precision: ', precision)\n",
        "  print('Recall: ', recall)\n",
        "  print('F-1 Score: ', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main(cap)"
      ],
      "metadata": {
        "id": "ehp5sGQST-QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_frame_cordinates(cap):\n",
        "  last_frame_cordinates = []\n",
        "\n",
        "  #Run code on input video - cap and store coordinates of all frames in dict_coordinates\n",
        "  main(cap)\n",
        "\n",
        "  #Return last frame coordinates\n",
        "  last_frame_cordinates.append(dict_coordinates['left_hand'][-1])\n",
        "  last_frame_cordinates.append(dict_coordinates['right_hand'][-1])\n",
        "  last_frame_cordinates.append(dict_coordinates['left_hip'][-1])\n",
        "  last_frame_cordinates.append(dict_coordinates['right_hip'][-1])\n",
        "  last_frame_cordinates.append(dict_coordinates['left_leg'][-1])\n",
        "  last_frame_cordinates.append(dict_coordinates['right_leg'][-1])\n",
        "  \n",
        "  return (last_frame_cordinates)"
      ],
      "metadata": {
        "id": "BxvttzLvT7mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RyzyRTJ7cv2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pose_Estimation.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}